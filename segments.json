"\n[\n    {\n        \"start\": \"00:00:00,000\",\n        \"end\": \"00:00:54,080\",\n        \"header\": \"Introduction and Overview of Previous Lecture\"\n    },\n    {\n        \"start\": \"00:00:54,080\",\n        \"end\": \"00:01:47,440\",\n        \"header\": \"Challenges of Bigram Model and Introduction to Multilayer Perceptron (MLP)\"\n    },\n    {\n        \"start\": \"00:01:47,440\",\n        \"end\": \"00:03:03,600\",\n        \"header\": \"Overview of Bengio et al. 2003 Paper\"\n    },\n    {\n        \"start\": \"00:03:03,600\",\n        \"end\": \"00:03:56,240\",\n        \"header\": \"Word-Level Versus Character-Level Language Models\"\n    },\n    {\n        \"start\": \"00:03:56,240\",\n        \"end\": \"00:07:03,560\",\n        \"header\": \"Character Embedding and Feature Representation in Neural Networks\"\n    },\n    {\n        \"start\": \"00:07:03,560\",\n        \"end\": \"00:09:17,440\",\n        \"header\": \"Neural Network Architecture: Input, Hidden Layer, and Output Layer\"\n    },\n    {\n        \"start\": \"00:09:17,440\",\n        \"end\": \"00:11:12,660\",\n        \"header\": \"Building the Dataset for Neural Network Training\"\n    },\n    {\n        \"start\": \"00:11:12,660\",\n        \"end\": \"00:14:52,400\",\n        \"header\": \"Embedding Lookup Table and Indexing in PyTorch\"\n    },\n    {\n        \"start\": \"00:14:52,400\",\n        \"end\": \"00:19:19,960\",\n        \"header\": \"Indexing Techniques and Multidimensional Indexing in PyTorch\"\n    },\n    {\n        \"start\": \"00:19:19,960\",\n        \"end\": \"00:22:06,160\",\n        \"header\": \"Constructing and Concatenating Embedding Vectors\"\n    },\n    {\n        \"start\": \"00:22:06,160\",\n        \"end\": \"00:23:19,360\",\n        \"header\": \"Efficient Tensor Reshaping Using .view() Method in PyTorch\"\n    },\n    {\n        \"start\": \"00:23:19,360\",\n        \"end\": \"00:23:57,520\",\n        \"header\": \"Implementing the Hidden Layer\"\n    },\n    {\n        \"start\": \"00:23:57,520\",\n        \"end\": \"00:28:16,560\",\n        \"header\": \"Understanding Broadcasting in PyTorch and Constructing Output Layer\"\n    },\n    {\n        \"start\": \"00:28:16,560\",\n        \"end\": \"00:32:05,420\",\n        \"header\": \"Computing Logits and Probability Distribution via Softmax\"\n    },\n    {\n        \"start\": \"00:32:05,420\",\n        \"end\": \"00:33:29,500\",\n        \"header\": \"Calculating Loss Using Cross-Entropy\"\n    },\n    {\n        \"start\": \"00:33:29,500\",\n        \"end\": \"00:39:02,720\",\n        \"header\": \"Training the Neural Network with Gradient Descent\"\n    },\n    {\n        \"start\": \"00:39:02,720\",\n        \"end\": \"00:44:02,600\",\n        \"header\": \"Optimizing Training via Mini-Batches\"\n    },\n    {\n        \"start\": \"00:44:02,600\",\n        \"end\": \"00:47:01,590\",\n        \"header\": \"Determining the Optimal Learning Rate\"\n    },\n    {\n        \"start\": \"00:47:01,590\",\n        \"end\": \"00:50:57,670\",\n        \"header\": \"Running Optimization and Learning Rate Decay\"\n    },\n    {\n        \"start\": \"00:50:57,670\",\n        \"end\": \"00:56:08,750\",\n        \"header\": \"Splitting Data into Training, Validation, and Test Sets\"\n    },\n    {\n        \"start\": \"00:56:08,750\",\n        \"end\": \"01:01:34,570\",\n        \"header\": \"Evaluating Model Performance and Avoiding Overfitting\"\n    },\n    {\n        \"start\": \"01:01:34,570\",\n        \"end\": \"01:04:42,830\",\n        \"header\": \"Increasing Model Capacity to Reduce Underfitting\"\n    },\n    {\n        \"start\": \"01:04:42,830\",\n        \"end\": \"01:07:01,510\",\n        \"header\": \"Training with Larger Embedding Sizes\"\n    },\n    {\n        \"start\": \"01:07:01,510\",\n        \"end\": \"01:12:16,670\",\n        \"header\": \"Implementing and Visualizing Character Embeddings\"\n    },\n    {\n        \"start\": \"01:12:16,670\",\n        \"end\": \"01:13:06,690\",\n        \"header\": \"Optimizing Model Further and Inviting Audience to Improve\"\n    },\n    {\n        \"start\": \"01:13:06,690\",\n        \"end\": \"01:14:37,670\",\n        \"header\": \"Sampling from the Trained Model\"\n    },\n    {\n        \"start\": \"01:14:37,670\",\n        \"end\": \"01:15:38,670\",\n        \"header\": \"Accessing and Running Code in Google Colab\"\n    }\n]\n"